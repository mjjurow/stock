{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ecfd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporal Graph Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb3502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28dc7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/rz03prk93_947fy2hsvpzyj80000gp/T/ipykernel_36550/3626376098.py:38: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  stocks.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Close', 'Volume', 'ticker', 'Change', '10min_MA', '60min_MA', '3hr_MA', '1day_MA', '5day_MA', 'hour_of_day', 'hour_of_day_normalized', 'hour_sin', 'hour_cos', 'Timestamp']\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"stock_data\"]\n",
    "collection = db[\"price_data\"]\n",
    "\n",
    "# Query all documents from the collection\n",
    "results = collection.find({})\n",
    "\n",
    "# Convert the query results to a pandas DataFrame\n",
    "data = pd.DataFrame(list(results))\n",
    "\n",
    "stocks = data.copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"_id\", \"Adj Close\"]\n",
    "stocks = stocks.drop(columns=columns_to_drop)\n",
    "\n",
    "# Create Change\n",
    "stocks['Change'] = stocks['Close'] - stocks['Open']\n",
    "\n",
    "# Drop 'Open' if it's no longer needed\n",
    "stocks = stocks.drop(columns=['Open', 'High', 'Low'])\n",
    "\n",
    "# Define rolling window sizes in minutes\n",
    "window_sizes = {\n",
    "    '10min': 10,\n",
    "    '60min': 60,\n",
    "    '3hr': 3 * 60,  # 3 hours in minutes\n",
    "    '1day': 24 * 60,  # 1 day in minutes\n",
    "    '5day': 5 * 24 * 60  # 5 days in minutes\n",
    "}\n",
    "\n",
    "# Calculate the moving averages for all stocks\n",
    "for window_name, minutes in window_sizes.items():\n",
    "    stocks[f'{window_name}_MA'] = stocks.groupby('ticker')['Close'].rolling(window=minutes, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Fill NaN values with the first available value if there are any NaNs\n",
    "stocks.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Convert 'Datetime' to Eastern Time\n",
    "eastern = timezone('US/Eastern')\n",
    "stocks['Datetime_ET'] = stocks['Datetime'].dt.tz_localize('UTC').dt.tz_convert(eastern)\n",
    "\n",
    "# Extract time features from 'Datetime_ET'\n",
    "stocks['hour_of_day'] = stocks['Datetime_ET'].dt.hour + stocks['Datetime_ET'].dt.minute / 60\n",
    "stocks['hour_of_day_normalized'] = 2 * np.pi * stocks['hour_of_day'] / 24\n",
    "stocks['hour_sin'] = np.sin(stocks['hour_of_day_normalized'])\n",
    "stocks['hour_cos'] = np.cos(stocks['hour_of_day_normalized'])\n",
    "\n",
    "# Now convert 'Datetime' to UNIX timestamp if needed\n",
    "stocks['Timestamp'] = stocks['Datetime'].astype('int64') // 1e9\n",
    "\n",
    "# Drop the original 'Datetime' and 'Datetime_ET' if they are no longer needed\n",
    "stocks = stocks.drop(columns=['Datetime', 'Datetime_ET'])\n",
    "\n",
    "stocks.sort_values(['ticker', 'Timestamp'], inplace=True)\n",
    "\n",
    "# Take 5 ticker symbols to validate code\n",
    "\n",
    "# List of tickers we want to keep\n",
    "tickers_to_keep = ['AAPL', 'ADBE', 'AMZN', 'MSFT', 'NVDA']\n",
    "\n",
    "# Create a new DataFrame with only the specified tickers\n",
    "five_stocks = stocks[stocks['ticker'].isin(tickers_to_keep)].copy()\n",
    "\n",
    "# Now, five_stocks contains only the data for the five specified tickers\n",
    "five_stocks.sort_values(['ticker', 'Timestamp'], inplace=True)\n",
    "five_stocks.head(5)\n",
    "\n",
    "# now have a dataframe with data for five stocks\n",
    "columns = five_stocks.columns.tolist()  # Convert Index to a list\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25347913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'five_stocks' has columns: 'timestamp', 'ticker', 'feature1', 'feature2', ..., 'Close'\n",
    "\n",
    "# Convert the timestamps to numerical format (e.g., seconds since some start time)\n",
    "five_stocks['Timestamp'] = pd.to_datetime(five_stocks['Timestamp'])\n",
    "five_stocks['Timestamp'] = (five_stocks['Timestamp'] - five_stocks['Timestamp'].min()).dt.total_seconds()\n",
    "\n",
    "# Encode tickers as node IDs\n",
    "ticker_to_id = {ticker: i for i, ticker in enumerate(five_stocks['ticker'].unique())}\n",
    "five_stocks['node_id'] = five_stocks['ticker'].map(ticker_to_id)\n",
    "\n",
    "# Here, we're making an assumption that each stock is connected to itself over time.\n",
    "# For a more complex network, you would define the edges based on your specific use case.\n",
    "edges = five_stocks[['node_id', 'node_id', 'Timestamp', 'Close']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fea240c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#live by the gpt...\n",
    "five_stocks['Timestamp'] = pd.to_datetime(five_stocks['Timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5798c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal\n",
    "\n",
    "# Create a list of edge_index, edge_weight, and feature matrices for each time step\n",
    "edge_indices = []\n",
    "edge_weights = []\n",
    "features = []\n",
    "targets = []\n",
    "\n",
    "# Group by daily data\n",
    "for day, day_df in five_stocks.groupby(five_stocks['Timestamp'].dt.floor('d')):\n",
    "    node_indices = day_df['node_id'].values\n",
    "    # Convert list of numpy arrays to a single numpy array before creating the tensor\n",
    "    edge_index = torch.tensor(np.array([node_indices, node_indices]), dtype=torch.long)\n",
    "    edge_weight = torch.tensor(day_df['Close'].values, dtype=torch.float)\n",
    "    feature = torch.tensor(day_df.drop(['Timestamp', 'ticker', 'node_id', 'Close'], axis=1).values, dtype=torch.float)\n",
    "    target = torch.tensor(day_df['Close'].values, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    edge_indices.append(edge_index)\n",
    "    edge_weights.append(edge_weight)\n",
    "    features.append(feature)\n",
    "    targets.append(target)\n",
    "    \n",
    "# Create a DynamicGraphTemporalSignal\n",
    "dataset = DynamicGraphTemporalSignal(edge_indices, edge_weights, features, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' THIS FAILS BECAUSE CANT IMPORT THE MODULE ON AN M2\n",
    "import torch\n",
    "from torch_geometric_temporal.nn.recurrent import TGNMemory, LSTM\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features, periods, memory_dimension, embedding_dimension):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.memory = TGNMemory(\n",
    "            memory_dimension=memory_dimension,\n",
    "            input_dimension=embedding_dimension,\n",
    "            message_dimension=embedding_dimension,\n",
    "            node_features=node_features,\n",
    "            periods=periods\n",
    "        )\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.gcn = GCNConv(embedding_dimension, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.memory(x, edge_index, None, edge_weight)\n",
    "        h = self.gcn(h, edge_index, edge_weight)\n",
    "        return h\n",
    "\n",
    "SO I HAVE TO DUMB IT DOWN\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e40229b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class LSTMGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features, lstm_hidden_dim, gcn_output_dim):\n",
    "        super(LSTMGCN, self).__init__()\n",
    "        self.lstm = nn.LSTM(node_features, lstm_hidden_dim, batch_first=True)\n",
    "        self.gcn = GCNConv(lstm_hidden_dim, gcn_output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Reshape for LSTM\n",
    "        # Assuming x is of shape (num_nodes, num_features)\n",
    "        x, _ = self.lstm(x.view(len(x), 1, -1))\n",
    "        \n",
    "        # Reshape back for GCN\n",
    "        x = x.view(len(x), -1)\n",
    "        \n",
    "        # Apply GCN\n",
    "        x = self.gcn(x, edge_index)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66ee52a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Close', 'Volume', 'ticker', 'Change', '10min_MA', '60min_MA', '3hr_MA', '1day_MA', '5day_MA', 'hour_of_day', 'hour_of_day_normalized', 'hour_sin', 'hour_cos', 'Timestamp', 'node_id']\n"
     ]
    }
   ],
   "source": [
    "columns = five_stocks.columns.tolist()  # Convert Index to a list\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b40891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features\n",
    "feature_columns = ['Volume', 'Change', '10min_MA', '60min_MA', '3hr_MA', \n",
    "                   '1day_MA', '5day_MA', 'hour_of_day', 'hour_of_day_normalized', \n",
    "                   'hour_sin', 'hour_cos']\n",
    "\n",
    "# Convert features and target to tensors\n",
    "features = torch.tensor(five_stocks[feature_columns].values, dtype=torch.float)\n",
    "targets = torch.tensor(five_stocks['Close'].values, dtype=torch.float).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e3f1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features, lstm_hidden_dim, gcn_output_dim):\n",
    "        super(LSTMGCN, self).__init__()\n",
    "        self.lstm = nn.LSTM(node_features, lstm_hidden_dim, batch_first=True)\n",
    "        self.gcn = GCNConv(lstm_hidden_dim, gcn_output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Reshape for LSTM\n",
    "        x, _ = self.lstm(x.view(len(x), 1, -1))\n",
    "        \n",
    "        # Reshape back for GCN\n",
    "        x = x.view(len(x), -1)\n",
    "        \n",
    "        # Apply GCN\n",
    "        x = self.gcn(x, edge_index)\n",
    "        return F.relu(x)\n",
    "\n",
    "# Initialize the model\n",
    "node_features = len(feature_columns)\n",
    "lstm_hidden_dim = 64  # You can adjust this\n",
    "gcn_output_dim = 1    # Assuming you want a single output value per node (e.g., predicted 'Close' value)\n",
    "\n",
    "model = LSTMGCN(node_features, lstm_hidden_dim, gcn_output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fc61b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate indices for splitting\n",
    "total_samples = features.shape[0]\n",
    "train_end = int(total_samples * 0.7)\n",
    "validate_end = int(total_samples * 0.85)\n",
    "\n",
    "# Split features and targets\n",
    "train_features, validate_features, test_features = features[:train_end], features[train_end:validate_end], features[validate_end:]\n",
    "train_targets, validate_targets, test_targets = targets[:train_end], targets[train_end:validate_end], targets[validate_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c40a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(num_nodes, k=5):\n",
    "    edge_index = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(max(0, i - k), min(num_nodes, i + k + 1)):\n",
    "            if i != j:\n",
    "                edge_index.append((i, j))\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create edges for each split\n",
    "train_edge_index = create_edges(train_features.shape[0])\n",
    "validate_edge_index = create_edges(validate_features.shape[0])\n",
    "test_edge_index = create_edges(test_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c06c5ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 110870.03125, Validation Loss: 138658.5\n",
      "Epoch 10, Train Loss: 108684.34375, Validation Loss: 136408.234375\n",
      "Epoch 20, Train Loss: 106973.9140625, Validation Loss: 133761.15625\n",
      "Epoch 30, Train Loss: 105669.515625, Validation Loss: 132056.40625\n",
      "Epoch 40, Train Loss: 104419.1953125, Validation Loss: 130432.3671875\n",
      "Epoch 50, Train Loss: 103081.5703125, Validation Loss: 128686.6796875\n",
      "Epoch 60, Train Loss: 101823.1875, Validation Loss: 127045.25\n",
      "Epoch 70, Train Loss: 100587.3515625, Validation Loss: 125432.0859375\n",
      "Epoch 80, Train Loss: 99379.8046875, Validation Loss: 123852.671875\n",
      "Epoch 90, Train Loss: 98198.671875, Validation Loss: 122304.3984375\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming your model is already defined and named 'model'\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 100  # Adjust as needed\n",
    "\n",
    "# Now the training and validation loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Clear the gradients\n",
    "    train_out = model(train_features, train_edge_index)  # Forward pass on train data\n",
    "    train_loss = criterion(train_out, train_targets)  # Compute the loss on train data\n",
    "    train_loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Validation step\n",
    "    if epoch % 10 == 0:  # For example, validate every 10 epochs\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            validate_out = model(validate_features, validate_edge_index)  # Forward pass on validation data\n",
    "            validate_loss = criterion(validate_out, validate_targets)  # Compute the loss on validation data\n",
    "            print(f'Epoch {epoch}, Train Loss: {train_loss.item()}, Validation Loss: {validate_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38424036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 174494.453125\n",
      "Predicted value: 21.80135154724121 - Actual value: 455.19281005859375\n",
      "Predicted value: 23.036176681518555 - Actual value: 455.0799865722656\n",
      "Predicted value: 24.211462020874023 - Actual value: 455.07000732421875\n",
      "Predicted value: 25.33391571044922 - Actual value: 455.1300048828125\n",
      "Predicted value: 26.409433364868164 - Actual value: 455.1650085449219\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    test_out = model(test_features, test_edge_index)  # Forward pass on test data\n",
    "    test_loss = criterion(test_out, test_targets)  # Compute the loss on test data\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "\n",
    "    # Optionally, compare the first few predicted and actual values\n",
    "    for i in range(min(5, test_out.size(0))):  # Just show the first 5 predictions\n",
    "        print(f'Predicted value: {test_out[i].item()} - Actual value: {test_targets[i].item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac053a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
